{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import emoji\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to obtain Whatsapp Chat data\n",
    "\n",
    "* Open whatsapp \n",
    "* Open a Group/Inbox\n",
    "* Click on the 3 dotted options button\n",
    "* Click on more\n",
    "* Click on export chat\n",
    "* Click on without media \n",
    "* Export via Email/other IM's/....\n",
    "* Download to your system rename to chat-data.txt and put it in a folder\n",
    "\n",
    "![](https://i.imgur.com/KldS1n5.png)\n",
    "\n",
    "\n",
    "```\n",
    "Without media: exports 40k messages \n",
    "With media: exports 10k messages along with pictures/videos \n",
    "As im are doing chat data analysis i went with `without media` option \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* Regex cheatsheet\n",
    "    * https://www.rexegg.com/regex-quickstart.html\n",
    "* Regex test - live\n",
    "    * https://regexr.com/\n",
    "* Datetime format\n",
    "    * http://strftime.org/\n",
    "    \n",
    "Use a custom a regex and datatime format by reffering to the above links if you run into empty df or format errors. As the exports from whatsapp are not standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_df(file, key):\n",
    "    split_formats = {\n",
    "        \"12hr\": \"\\d{1,2}/\\d{1,2}/\\d{2,4},\\s\\d{1,2}:\\d{2}\\s[APap][mM]\\s-\\s\",\n",
    "        \"24hr\": \"\\d{1,2}/\\d{1,2}/\\d{2,4},\\s\\d{1,2}:\\d{2}\\s-\\s\",\n",
    "        \"custom\": \"\",\n",
    "    }\n",
    "    datetime_formats = {\n",
    "        \"12hr\": \"%m/%d/%Y, %I:%M %p - \",\n",
    "        \"24hr\": \"%m/%d/%y, %H:%M - \",\n",
    "        \"custom\": \"\",\n",
    "    }\n",
    "    # import codecs\n",
    "    types_of_encoding = [\n",
    "        \"utf-8\"\n",
    "    ]  # \"utf-8\"#,\"cp1252\",\"utf8\",\"cp850\",\"utf-16-le\",\"utf-32-le\"]\n",
    "    for encoding_type in types_of_encoding:\n",
    "        with codecs.open(file, encoding=encoding_type, errors=\"strict\") as raw_data:\n",
    "            raw_string = \" \".join(raw_data.read().split(\"\\n\"))\n",
    "            user_msg = re.split(split_formats[key], raw_string)[1:]\n",
    "            date_time = re.findall(split_formats[key], raw_string)\n",
    "            df_ = pd.DataFrame({\"date_time\": date_time, \"user_msg\": user_msg})\n",
    "    # converting date-time pattern which is of type String to type datetime,\n",
    "    # format is to be specified for the whole string where the placeholders are extracted by the method\n",
    "    df_[\"date_time\"] = pd.to_datetime(df_[\"date_time\"], format=datetime_formats[key])\n",
    "    # split user and msg\n",
    "    usernames = []\n",
    "    msgs = []\n",
    "    for element in df_[\"user_msg\"]:\n",
    "        matched_pattern_list = re.split(\n",
    "            \"([\\w\\W]+?):\\s\", element\n",
    "        )  # lazy pattern match to first {user_name}: pattern and spliting it aka each msg from a user\n",
    "        if matched_pattern_list[1:]:  # user typed messages\n",
    "            usernames.append(matched_pattern_list[1])\n",
    "            msgs.append(matched_pattern_list[2])\n",
    "        else:  # other notifications in the group(eg: someone was added, some left ...)\n",
    "            usernames.append(\"grp_notif\")\n",
    "            msgs.append(matched_pattern_list[0])\n",
    "    # creating new columns\n",
    "    df_[\"user\"] = usernames\n",
    "    df_[\"msg\"] = msgs\n",
    "    # dropping the old user_msg col.\n",
    "    df.drop(\"user_msg\", axis=1, inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = input(\"Enter txt file name: \")\n",
    "df = raw_to_df(file_, \"24hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape) # no of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_name = input(\"Enter user name to run analytics: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df[\n",
    "    df[\"msg\"] == \"<Media omitted> \"\n",
    "]  # no. of images, images are represented by <media omitted>\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"user\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_notif = df[df[\"user\"] == \"grp_notif\"]  # no. of grp notifications\n",
    "print(grp_notif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(images.index, inplace=True)  # removing images\n",
    "df.drop(grp_notif.index, inplace=True)  # removing grp_notif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most active / least active members of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"total messages per {df.groupby('user')['msg'].count().sort_values(ascending=False)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emoji count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_ctr = Counter()\n",
    "emojis_list = map(lambda x: \"\".join(x.split()), emoji.UNICODE_EMOJI.keys())\n",
    "r = re.compile(\"|\".join(re.escape(p) for p in emojis_list))\n",
    "for idx, row in df.iterrows():\n",
    "    if row[\"user\"] == usr_name:\n",
    "        emojis_found = r.findall(row[\"msg\"])\n",
    "        for emoji_found in emojis_found:\n",
    "            emoji_ctr[emoji_found] += 1\n",
    "for item in emoji_ctr.most_common(10):\n",
    "    print(item[0] + \" - \" + str(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single user time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"date_time\"].apply(lambda x: x.hour)\n",
    "df[df[\"user\"] == usr_name].groupby([\"hour\"]).size().sort_index().plot(\n",
    "    x=\"hour\", kind=\"bar\", title=usr_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User message count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df[\"date_time\"].apply(\n",
    "    lambda x: x.day_name()\n",
    ")  # can use day_name or weekday from datetime\n",
    "df[\"is_weekend\"] = df.weekday.isin([\"Sunday\", \"Saturday\"])\n",
    "msgs_per_user = df[\"user\"].value_counts(sort=True)\n",
    "print(msgs_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top n Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = input(\"Enter how many top n users for analysis?: \")\n",
    "n_users = int(n_users)\n",
    "top_n_users = msgs_per_user.index.tolist()[:n_users]\n",
    "print(top_n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_n = df.copy()\n",
    "df_top_n = df_top_n[df_top_n.user.isin(top_n_users)]\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.countplot(x=\"user\", hue=\"weekday\", data=df_top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekend vs Weekday Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_n[\"is_weekend\"] = df_top_n.weekday.isin([\"Sunday\", \"Saturday\"])\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(x=\"user\", hue=\"is_weekend\", data=df_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(val_):\n",
    "    return len(val_.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"no_of_words\"] = df[\"msg\"].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_n[\"no_of_words\"] = df_top_n[\"msg\"].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_weekday = df[df[\"is_weekend\"] is False][\"no_of_words\"].sum()\n",
    "print(total_words_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_weekend = df[df[\"is_weekend\"]][\"no_of_words\"].sum()\n",
    "print(total_words_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"average words on a weekday: {total_words_weekday / 5}\"\n",
    ")  # average words on a weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"average words on a weekend: {total_words_weekend / 2}\"\n",
    ")  # average words on a weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"user\")[\"no_of_words\"].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_top_n.groupby(\"user\")[\"no_of_words\"].sum() / df_top_n.groupby(\"user\").size()\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPerMsg_weekday_vs_weekend = (\n",
    "    df_top_n.groupby([\"user\", \"is_weekend\"])[\"no_of_words\"].sum()\n",
    "    / df_top_n.groupby([\"user\", \"is_weekend\"]).size()\n",
    ")\n",
    "print(wordPerMsg_weekday_vs_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordPerMsg_weekday_vs_weekend.plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 5)\n",
    "## Most Usage - Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_per_time = df.groupby([\"hour\", \"weekday\"])[\"msg\"].size().reset_index()\n",
    "pivot_msg_per_time = msg_per_time.pivot(\"hour\", \"weekday\", \"msg\")\n",
    "print(pivot_msg_per_time.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "sns.heatmap(pivot_msg_per_time[days].fillna(0), cmap=\"YlGnBu\", robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 6)\n",
    "## In any group, do I have any inclination towards responding to someone? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_msgs_index = np.array(df[df[\"user\"] == usr_name].index)\n",
    "print(my_msgs_index, my_msgs_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_msgs_index = my_msgs_index - 1\n",
    "print(prev_msgs_index, prev_msgs_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies = df.iloc[prev_msgs_index].copy()\n",
    "print(df_replies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies.groupby([\"user\"])[\"msg\"].size().sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 7)\n",
    "## Which are the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMMENT_WORDS = \" \"\n",
    "stopwords = STOPWORDS.update(\n",
    "    [\n",
    "        \"lo\",\n",
    "        \"hai\",\n",
    "        \"ge\",\n",
    "        \"Lo\",\n",
    "        \"illa\",\n",
    "        \"yea\",\n",
    "        \"ella\",\n",
    "        \"en\",\n",
    "        \"na\",\n",
    "        \"En\",\n",
    "        \"yeah\",\n",
    "        \"alli\",\n",
    "        \"ide\",\n",
    "        \"okay\",\n",
    "        \"ok\",\n",
    "        \"will\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for val in df.msg.values:\n",
    "    val = arabic_reshaper.reshape(str(val))\n",
    "    val = get_display(val)\n",
    "    tokens = val.split()\n",
    "    for i in enumerate(tokens):\n",
    "        tokens[i[0]] = tokens[i[0]].lower()\n",
    "    for words in tokens:\n",
    "        COMMENT_WORDS = COMMENT_WORDS + words + \" \"\n",
    "wordcloud = WordCloud(\n",
    "    font_path=\"arial\",\n",
    "    width=800,\n",
    "    height=800,\n",
    "    background_color=\"black\",\n",
    "    stopwords=stopwords,\n",
    "    min_font_size=10,\n",
    ").generate(COMMENT_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
